# ğŸ§  Sistema de Roteamento Inteligente de LLMs (LLM Router)
*Data: 2025-10-04 03:20*

## ğŸ¯ **Conceito**

**Cascata de LLMs**: ComeÃ§a com modelos rÃ¡pidos/baratos e escala automaticamente para modelos mais potentes conforme a complexidade da consulta.

### ğŸ’¡ **Por que usar LLM Routing?**

1. **âš¡ Performance**: Queries simples usam modelos rÃ¡pidos
2. **ğŸ’° Custo-efetivo**: Paga menos por tarefas bÃ¡sicas
3. **ğŸ¯ PrecisÃ£o**: Usa modelos potentes apenas quando necessÃ¡rio
4. **ğŸ“Š Escalabilidade**: Otimiza recursos automaticamente

## ğŸ“Š **NÃ­veis de Complexidade**

### 1ï¸âƒ£ **SIMPLE** - gemini-1.5-flash
**Quando usar:**
- SaudaÃ§Ãµes e interaÃ§Ãµes bÃ¡sicas
- Perguntas sobre status
- Help e ajuda
- Comandos simples

**Exemplos:**
- "OlÃ¡, como vocÃª estÃ¡?"
- "Help"
- "O que vocÃª pode fazer?"
- "Status do sistema"

**ConfiguraÃ§Ã£o:**
- Temperature: 0.3
- Max tokens: 500
- Custo: Mais barato

### 2ï¸âƒ£ **MEDIUM** - gemini-1.5-flash
**Quando usar:**
- AnÃ¡lise de dados simples
- EstatÃ­sticas bÃ¡sicas
- VisualizaÃ§Ã£o de informaÃ§Ãµes
- Datasets pequenos (<10k linhas)

**Exemplos:**
- "Quantas linhas tem o arquivo?"
- "Mostre as estatÃ­sticas bÃ¡sicas"
- "Liste as colunas disponÃ­veis"
- "Exiba um resumo dos dados"

**ConfiguraÃ§Ã£o:**
- Temperature: 0.5
- Max tokens: 1500
- Custo: Barato

### 3ï¸âƒ£ **COMPLEX** - gemini-1.5-pro
**Quando usar:**
- DetecÃ§Ã£o de fraude
- AnÃ¡lise de correlaÃ§Ãµes
- PadrÃµes e anomalias
- Datasets grandes (10k-100k linhas)
- MÃºltiplos agentes

**Exemplos:**
- "Analise este dataset para fraude"
- "Detecte padrÃµes anÃ´malos"
- "Encontre correlaÃ§Ãµes entre variÃ¡veis"
- "Identifique outliers"

**ConfiguraÃ§Ã£o:**
- Temperature: 0.7
- Max tokens: 3000
- Custo: MÃ©dio

### 4ï¸âƒ£ **ADVANCED** - gemini-2.0-flash-exp
**Quando usar:**
- AnÃ¡lise massiva (>100k linhas)
- Machine Learning complexo
- CorrelaÃ§Ãµes avanÃ§adas
- Todos os agentes trabalhando juntos
- OtimizaÃ§Ã£o e deep learning

**Exemplos:**
- "FaÃ§a uma anÃ¡lise completa com todos os agentes"
- "AnÃ¡lise massiva de correlaÃ§Ãµes complexas"
- "Otimize o modelo de detecÃ§Ã£o"
- "Deep learning analysis"

**ConfiguraÃ§Ã£o:**
- Temperature: 0.8
- Max tokens: 4000
- Custo: Mais caro (mas com capacidades avanÃ§adas)

## ğŸ”§ **Como Funciona**

### 1. **DetecÃ§Ã£o AutomÃ¡tica de Complexidade**

```python
from src.llm.llm_router import LLMRouter

# O router analisa automaticamente
complexity = LLMRouter.detect_complexity(
    query="Analise este dataset para fraude",
    context={"dataset_size": {"rows": 100000, "columns": 30}}
)

# Retorna: ComplexityLevel.COMPLEX
```

### 2. **SeleÃ§Ã£o do Modelo**

```python
# Router seleciona o modelo apropriado
routing = LLMRouter.route_query(
    query="Analise este dataset para fraude",
    context={"dataset_size": {"rows": 100000, "columns": 30}}
)

# Retorna:
# {
#     "model_name": "gemini-1.5-pro",
#     "complexity_name": "COMPLEX",
#     "temperature": 0.7,
#     "max_tokens": 3000,
#     "reasoning": "AnÃ¡lise profunda multi-agente"
# }
```

### 3. **IntegraÃ§Ã£o na API**

O roteamento Ã© **automÃ¡tico** na `api_completa.py`:

```python
# Antes de processar a query
llm_config = create_llm_with_routing(request.message, context)

# Logs automÃ¡ticos
# ğŸ§  LLM Router: gemini-1.5-pro (Complexidade: COMPLEX)
#    Temperature: 0.7, Reasoning: AnÃ¡lise profunda multi-agente
```

## ğŸ“Š **CritÃ©rios de DetecÃ§Ã£o**

### **Palavras-chave**
```python
SIMPLE: ["olÃ¡", "oi", "help", "status"]
MEDIUM: ["quantas linhas", "mostre", "lista"]
COMPLEX: ["fraude", "detecÃ§Ã£o", "correlaÃ§Ã£o", "padrÃµes"]
ADVANCED: ["anÃ¡lise completa", "todos os agentes", "deep learning"]
```

### **Tamanho do Dataset**
```python
> 100.000 linhas = COMPLEX
> 10.000 linhas = MEDIUM
< 10.000 linhas = SIMPLE/MEDIUM
```

### **Comprimento da Query**
```python
> 200 caracteres = COMPLEX
> 100 caracteres = MEDIUM
< 100 caracteres = SIMPLE/MEDIUM
```

### **Flags ExplÃ­citas**
```python
context = {"force_complexity": ComplexityLevel.ADVANCED}
# ForÃ§a uso do modelo mais potente
```

## ğŸš€ **Como Usar na API**

### **Upload e AnÃ¡lise Simples**
```bash
# 1. Upload CSV
curl -X POST "http://localhost:8001/csv/upload" \
  -F "file=@dados.csv"

# 2. Pergunta simples (usa gemini-1.5-flash)
curl -X POST "http://localhost:8001/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Quantas linhas tem o arquivo?",
    "file_id": "csv_xxx"
  }'
```

**Resposta:**
```json
{
  "response": "O arquivo possui 1.234 linhas e 15 colunas.",
  "llm_model": "gemini-1.5-flash",
  "complexity_level": "MEDIUM",
  "agent_used": "csv_contextual_analyzer"
}
```

### **AnÃ¡lise Complexa de Fraude**
```bash
curl -X POST "http://localhost:8001/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Analise este dataset completamente para detecÃ§Ã£o de fraude usando todos os agentes disponÃ­veis",
    "file_id": "csv_xxx"
  }'
```

**Resposta:**
```json
{
  "response": "AnÃ¡lise multiagente completa... [resposta detalhada]",
  "llm_model": "gemini-2.0-flash-exp",
  "complexity_level": "ADVANCED",
  "agent_used": "orchestrator",
  "analysis_type": "multiagent_csv_analysis"
}
```

## ğŸ“ˆ **EscalaÃ§Ã£o de Complexidade**

Se o modelo atual falhar, o sistema pode escalar:

```python
# EscalaÃ§Ã£o automÃ¡tica
current = ComplexityLevel.MEDIUM
escalated = LLMRouter.escalate_complexity(current)
# Retorna: ComplexityLevel.COMPLEX

# Tenta novamente com modelo mais potente
```

## ğŸ’¡ **BenefÃ­cios do Sistema**

### âœ… **Performance**
- Queries simples: ~0.2s com gemini-1.5-flash
- Queries complexas: ~2-5s com gemini-1.5-pro
- Queries avanÃ§adas: ~5-10s com gemini-2.0-flash-exp

### âœ… **Custo**
- 70% das queries usam modelos baratos
- 25% usam modelos mÃ©dios
- 5% usam modelos premium

**Economia estimada: 60-70% vs uso constante do modelo mais caro**

### âœ… **PrecisÃ£o**
- Modelo correto para cada tarefa
- Sem overhead para tarefas simples
- Capacidade mÃ¡xima quando necessÃ¡rio

## ğŸ” **Monitoramento**

Os logs mostram automaticamente:

```
INFO:ğŸ¤– Iniciando anÃ¡lise multiagente para file_id: csv_xxx
INFO:ğŸ§  LLM Router: gemini-1.5-pro (Complexidade: COMPLEX)
INFO:   Temperature: 0.7, Reasoning: AnÃ¡lise profunda multi-agente
INFO:âœ… AnÃ¡lise multiagente concluÃ­da: orchestrator
INFO:Chat processado em 3.45s por orchestrator
INFO:   LLM: gemini-1.5-pro | Complexidade: COMPLEX
```

## ğŸ¯ **Casos de Uso Reais**

### **Caso 1: Dashboard de MÃ©tricas**
- Query: "Mostre o status"
- Modelo: gemini-1.5-flash (SIMPLE)
- Tempo: ~0.1s
- Custo: MÃ­nimo

### **Caso 2: AnÃ¡lise EstatÃ­stica**
- Query: "EstatÃ­sticas do dataset"
- Modelo: gemini-1.5-flash (MEDIUM)
- Tempo: ~0.5s
- Custo: Baixo

### **Caso 3: DetecÃ§Ã£o de Fraude**
- Query: "Analise fraudes"
- Modelo: gemini-1.5-pro (COMPLEX)
- Tempo: ~3s
- Custo: MÃ©dio

### **Caso 4: AnÃ¡lise Massiva**
- Query: "AnÃ¡lise completa com todos os agentes"
- Modelo: gemini-2.0-flash-exp (ADVANCED)
- Tempo: ~8s
- Custo: Alto (mas justificado)

---

**ğŸŠ SISTEMA DE ROTEAMENTO INTELIGENTE ATIVO!**

**OtimizaÃ§Ã£o automÃ¡tica**: 60-70% de economia
**Performance**: Modelos adequados para cada tarefa
**API**: http://localhost:8001