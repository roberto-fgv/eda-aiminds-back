#!/usr/bin/env python3
"""
Teste do Sistema de Fallback LLM
=================================

Este script testa especificamente o sistema de fallback:
Grok (primary) -> Google LLM (fallback) no OrchestratorAgent.
"""

from __future__ import annotations
import sys
from pathlib import Path

# Adiciona o diretÃ³rio raiz do projeto ao PYTHONPATH
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

def test_orchestrator_fallback():
    """Testa o sistema de fallback no orquestrador."""
    print("ğŸ§ª TESTE DO SISTEMA DE FALLBACK LLM")
    print("=" * 50)
    
    try:
        # Verificar configuraÃ§Ãµes
        from src.settings import GROK_API_KEY, GOOGLE_API_KEY
        print(f"ğŸ”‘ GROK_API_KEY: {'âœ… Configurado' if GROK_API_KEY else 'âŒ Ausente'}")
        print(f"ğŸ”‘ GOOGLE_API_KEY: {'âœ… Configurado' if GOOGLE_API_KEY else 'âŒ Ausente'}")
        
        # Inicializar orquestrador
        print("\nğŸ”§ Inicializando OrchestratorAgent...")
        from src.agent.orchestrator_agent import OrchestratorAgent
        
        orchestrator = OrchestratorAgent(
            enable_csv_agent=False,        # Desabilitar para simplicidade
            enable_rag_agent=False,        # Desabilitar para simplicidade  
            enable_google_llm_agent=True,  # Habilitar Google como fallback
            enable_grok_llm_agent=True,    # Habilitar Grok como primary
            enable_data_processor=False    # Desabilitar para simplicidade
        )
        
        # Verificar agentes disponÃ­veis
        agents = list(orchestrator.agents.keys())  
        print(f"âœ… Agentes inicializados: {', '.join(agents)}")
        
        if "grok" in agents:
            print("ğŸ¯ Grok LLM disponÃ­vel (primary)")
        if "llm" in agents:
            print("ğŸ“± Google LLM disponÃ­vel (fallback)")
        
        # Testar consulta que aciona LLM
        test_query = "Explique os principais indicadores de fraude em transaÃ§Ãµes financeiras"
        print(f"\nğŸ“ Consulta: {test_query}")
        
        print("ğŸš€ Processando consulta...")
        result = orchestrator.process(test_query)
        
        # Analisar resultado
        metadata = result.get('metadata', {})
        agents_used = metadata.get('agents_used', [])
        success = metadata.get('success', False)
        
        print(f"\nğŸ“Š RESULTADO:")
        print(f"   âœ… Sucesso: {success}")
        print(f"   ğŸ¤– Agentes usados: {', '.join(agents_used)}")
        
        if 'grok' in agents_used:
            print("   ğŸ¯ Grok foi utilizado! (Primary LLM)")
        elif 'llm' in agents_used:
            print("   ğŸ“± Google LLM foi utilizado! (Fallback funcionou)")
        else:
            print("   âš ï¸ Nenhum LLM foi utilizado")
        
        content = result.get('content', '')
        if content and success:
            print(f"\nğŸ’¬ Resposta (primeiros 200 chars):")
            print(f"   {content[:200]}...")
        elif not success:
            print(f"\nâŒ Erro: {content}")
        
        return success
        
    except Exception as e:
        print(f"âŒ Erro no teste de fallback: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_llm_classification():
    """Testa se o orquestrador classifica corretamente consultas para LLM."""
    print("\nğŸ§ª TESTE DE CLASSIFICAÃ‡ÃƒO DE CONSULTAS")
    print("=" * 50)
    
    try:
        from src.agent.orchestrator_agent import OrchestratorAgent
        orchestrator = OrchestratorAgent(
            enable_csv_agent=False,
            enable_rag_agent=False,
            enable_google_llm_agent=True,
            enable_grok_llm_agent=True,
            enable_data_processor=False
        )
        
        # Queries que devem acionar LLM
        llm_queries = [
            "explique padrÃµes de fraude",
            "analise correlaÃ§Ãµes nos dados", 
            "como interpretar anomalias",
            "quais sÃ£o os principais indicadores de risco"
        ]
        
        for i, query in enumerate(llm_queries, 1):
            print(f"\n{i}ï¸âƒ£ Query: {query}")
            
            # Classificar consulta (mÃ©todo interno)
            query_type = orchestrator._classify_query(query)
            print(f"   ğŸ“‹ ClassificaÃ§Ã£o: {query_type.value}")
            
            if query_type.value == "llm_analysis":
                print("   âœ… Corretamente classificada para LLM")
            else:
                print("   âš ï¸ NÃ£o classificada para LLM")
        
    except Exception as e:
        print(f"âŒ Erro na classificaÃ§Ã£o: {e}")

def main():
    """Executa testes do sistema de fallback."""
    print("ğŸš€ INICIANDO TESTES DO SISTEMA DE FALLBACK")
    print("=" * 60)
    
    # Teste 1: Fallback functionality
    success = test_orchestrator_fallback()
    
    # Teste 2: Query classification  
    test_llm_classification()
    
    print("\nğŸ TESTES CONCLUÃDOS")
    print("=" * 60)
    
    if success:
        print("âœ… Sistema de fallback funcionando!")
    else:
        print("âš ï¸ Verificar configuraÃ§Ãµes de API keys")

if __name__ == "__main__":
    main()