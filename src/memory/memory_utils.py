"""
M√≥dulo de utilit√°rios para o sistema de mem√≥ria dos agentes.

Este m√≥dulo fornece fun√ß√µes utilit√°rias, helpers e ferramentas de apoio
para o sistema de mem√≥ria persistente dos agentes multiagente.
"""

import hashlib
import json
import sys
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple, Union
from uuid import UUID, uuid4

from .memory_types import (
    AgentContext,
    ContextType,
    ConversationMessage,
    EmbeddingType,
    MemoryConfig,
    MessageType,
    SessionInfo,
    SessionStatus,
    SessionType
)


# ============================================================================
# GERADORES DE ID E HASH
# ============================================================================

def generate_session_id(prefix: str = "session", include_timestamp: bool = True) -> str:
    """
    Gera um ID √∫nico para sess√£o.
    
    Args:
        prefix: Prefixo para o ID
        include_timestamp: Se deve incluir timestamp no ID
        
    Returns:
        String com ID √∫nico da sess√£o
    """
    unique_part = str(uuid4())[:8]
    
    if include_timestamp:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"{prefix}_{timestamp}_{unique_part}"
    else:
        return f"{prefix}_{unique_part}"


def generate_context_hash(agent_name: str, context_type: str, context_data: Dict[str, Any]) -> str:
    """
    Gera hash √∫nico para contexto baseado no conte√∫do.
    
    Args:
        agent_name: Nome do agente
        context_type: Tipo do contexto
        context_data: Dados do contexto
        
    Returns:
        Hash MD5 do contexto
    """
    content_str = f"{agent_name}_{context_type}_{json.dumps(context_data, sort_keys=True)}"
    return hashlib.md5(content_str.encode()).hexdigest()


def generate_content_fingerprint(content: str) -> str:
    """
    Gera fingerprint de conte√∫do para detec√ß√£o de duplicatas.
    
    Args:
        content: Conte√∫do para gerar fingerprint
        
    Returns:
        Hash SHA256 truncado do conte√∫do
    """
    return hashlib.sha256(content.encode()).hexdigest()[:16]


# ============================================================================
# UTILIT√ÅRIOS DE DATA E TEMPO
# ============================================================================

def calculate_session_expiry(session_type: SessionType, custom_hours: Optional[int] = None) -> datetime:
    """
    Calcula data de expira√ß√£o baseada no tipo de sess√£o.
    
    Args:
        session_type: Tipo da sess√£o
        custom_hours: Dura√ß√£o customizada em horas
        
    Returns:
        Data/hora de expira√ß√£o
    """
    if custom_hours:
        hours = min(custom_hours, MemoryConfig.MAX_SESSION_DURATION_HOURS)
    else:
        # Dura√ß√µes padr√£o por tipo
        duration_map = {
            SessionType.INTERACTIVE: 24,
            SessionType.BATCH: 2,
            SessionType.API: 12,
            SessionType.SYSTEM: 24 * 7  # 1 semana
        }
        hours = duration_map.get(session_type, MemoryConfig.DEFAULT_SESSION_DURATION_HOURS)
    
    return datetime.now() + timedelta(hours=hours)


def is_within_retention_period(created_at: datetime, retention_days: int) -> bool:
    """
    Verifica se est√° dentro do per√≠odo de reten√ß√£o.
    
    Args:
        created_at: Data de cria√ß√£o
        retention_days: Dias de reten√ß√£o
        
    Returns:
        True se ainda est√° no per√≠odo de reten√ß√£o
    """
    cutoff_date = datetime.now() - timedelta(days=retention_days)
    return created_at > cutoff_date


def format_duration(start_time: datetime, end_time: Optional[datetime] = None) -> str:
    """
    Formata dura√ß√£o entre dois timestamps.
    
    Args:
        start_time: Tempo inicial
        end_time: Tempo final (padr√£o: agora)
        
    Returns:
        String formatada da dura√ß√£o
    """
    if end_time is None:
        end_time = datetime.now()
    
    delta = end_time - start_time
    
    if delta.days > 0:
        return f"{delta.days}d {delta.seconds // 3600}h"
    elif delta.seconds >= 3600:
        return f"{delta.seconds // 3600}h {(delta.seconds % 3600) // 60}m"
    elif delta.seconds >= 60:
        return f"{(delta.seconds % 3600) // 60}m {delta.seconds % 60}s"
    else:
        return f"{delta.seconds}s"


# ============================================================================
# UTILIT√ÅRIOS DE TAMANHO E COMPRESS√ÉO
# ============================================================================

def calculate_data_size(data: Any) -> int:
    """
    Calcula tamanho aproximado de dados em bytes.
    
    Args:
        data: Dados para calcular tamanho
        
    Returns:
        Tamanho em bytes
    """
    if isinstance(data, str):
        return len(data.encode('utf-8'))
    elif isinstance(data, dict) or isinstance(data, list):
        return len(json.dumps(data).encode('utf-8'))
    else:
        return sys.getsizeof(data)


def truncate_content(content: str, max_length: int = MemoryConfig.MAX_MESSAGE_LENGTH) -> str:
    """
    Trunca conte√∫do se exceder tamanho m√°ximo.
    
    Args:
        content: Conte√∫do para truncar
        max_length: Tamanho m√°ximo permitido
        
    Returns:
        Conte√∫do truncado se necess√°rio
    """
    if len(content) <= max_length:
        return content
    
    truncated = content[:max_length - 50]  # Reserva espa√ßo para indicador
    return f"{truncated}... [TRUNCADO - {len(content)} caracteres originais]"


def compress_old_conversations(messages: List[ConversationMessage], max_turns: int = 50) -> List[ConversationMessage]:
    """
    Comprime conversa√ß√µes antigas mantendo apenas mensagens mais relevantes.
    
    Args:
        messages: Lista de mensagens
        max_turns: M√°ximo de turnos a manter
        
    Returns:
        Lista comprimida de mensagens
    """
    if len(messages) <= max_turns:
        return messages
    
    # Manter sempre as primeiras e √∫ltimas mensagens
    first_msgs = messages[:5]
    last_msgs = messages[-(max_turns - 10):]
    
    # Criar mensagem de sum√°rio
    compressed_count = len(messages) - len(first_msgs) - len(last_msgs)
    summary_msg = ConversationMessage(
        agent_name="system",
        message_type=MessageType.SYSTEM,
        content=f"[SUM√ÅRIO: {compressed_count} mensagens comprimidas]",
        metadata={"compressed": True, "original_count": compressed_count}
    )
    
    return first_msgs + [summary_msg] + last_msgs


# ============================================================================
# UTILIT√ÅRIOS DE VALIDA√á√ÉO E SANITIZA√á√ÉO
# ============================================================================

def sanitize_agent_name(agent_name: str) -> str:
    """
    Sanitiza nome do agente para uso seguro.
    
    Args:
        agent_name: Nome original do agente
        
    Returns:
        Nome sanitizado
    """
    # Remove caracteres especiais e limita tamanho
    sanitized = ''.join(c for c in agent_name if c.isalnum() or c in ['_', '-'])
    return sanitized[:100].lower()


def sanitize_session_id(session_id: str) -> str:
    """
    Sanitiza session ID para uso seguro.
    
    Args:
        session_id: ID original da sess√£o
        
    Returns:
        Session ID sanitizado
    """
    # Remove caracteres especiais exceto alguns permitidos
    sanitized = ''.join(c for c in session_id if c.isalnum() or c in ['_', '-', '.'])
    return sanitized[:255]


def validate_context_data(context_data: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
    """
    Valida dados de contexto.
    
    Args:
        context_data: Dados do contexto para validar
        
    Returns:
        Tuple (√©_v√°lido, mensagem_erro)
    """
    if not isinstance(context_data, dict):
        return False, "Context data deve ser um dicion√°rio"
    
    # Verifica tamanho
    size = calculate_data_size(context_data)
    if size > MemoryConfig.MAX_CONTEXT_SIZE_BYTES:
        return False, f"Context data muito grande: {size} bytes (m√°ximo: {MemoryConfig.MAX_CONTEXT_SIZE_BYTES})"
    
    # Verifica se √© serializ√°vel
    try:
        json.dumps(context_data)
    except (TypeError, ValueError) as e:
        return False, f"Context data n√£o √© serializ√°vel: {str(e)}"
    
    return True, None


def clean_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    Limpa e padroniza metadados.
    
    Args:
        metadata: Metadados originais
        
    Returns:
        Metadados limpos
    """
    if not isinstance(metadata, dict):
        return {}
    
    cleaned = {}
    for key, value in metadata.items():
        # Limita chaves e converte valores n√£o serializ√°veis
        if isinstance(key, str) and len(key) <= 50:
            try:
                json.dumps(value)  # Testa se √© serializ√°vel
                cleaned[key] = value
            except (TypeError, ValueError):
                cleaned[key] = str(value)  # Converte para string se n√£o for serializ√°vel
    
    return cleaned


# ============================================================================
# UTILIT√ÅRIOS DE BUSCA E FILTRO
# ============================================================================

def filter_sessions_by_status(sessions: List[SessionInfo], status: SessionStatus) -> List[SessionInfo]:
    """
    Filtra sess√µes por status.
    
    Args:
        sessions: Lista de sess√µes
        status: Status para filtrar
        
    Returns:
        Lista filtrada de sess√µes
    """
    return [session for session in sessions if session.status == status]


def filter_expired_sessions(sessions: List[SessionInfo]) -> List[SessionInfo]:
    """
    Filtra sess√µes expiradas.
    
    Args:
        sessions: Lista de sess√µes
        
    Returns:
        Lista de sess√µes expiradas
    """
    return [session for session in sessions if session.is_expired()]


def group_contexts_by_type(contexts: List[AgentContext]) -> Dict[ContextType, List[AgentContext]]:
    """
    Agrupa contextos por tipo.
    
    Args:
        contexts: Lista de contextos
        
    Returns:
        Dicion√°rio agrupado por tipo
    """
    grouped = {}
    for context in contexts:
        context_type = context.context_type
        if context_type not in grouped:
            grouped[context_type] = []
        grouped[context_type].append(context)
    
    return grouped


def find_recent_conversations(messages: List[ConversationMessage], hours: int = 24) -> List[ConversationMessage]:
    """
    Encontra conversa√ß√µes recentes.
    
    Args:
        messages: Lista de mensagens
        hours: N√∫mero de horas para considerar "recente"
        
    Returns:
        Lista de mensagens recentes
    """
    cutoff_time = datetime.now() - timedelta(hours=hours)
    return [msg for msg in messages if msg.timestamp > cutoff_time]


# ============================================================================
# UTILIT√ÅRIOS DE EMBEDDINGS
# ============================================================================

def normalize_embedding(embedding: List[float]) -> List[float]:
    """
    Normaliza embedding para magnitude unit√°ria.
    
    Args:
        embedding: Lista de valores do embedding
        
    Returns:
        Embedding normalizado
    """
    import math
    
    magnitude = math.sqrt(sum(x * x for x in embedding))
    if magnitude == 0:
        return embedding
    
    return [x / magnitude for x in embedding]


def calculate_cosine_similarity(embedding1: List[float], embedding2: List[float]) -> float:
    """
    Calcula similaridade cosseno entre dois embeddings.
    
    Args:
        embedding1: Primeiro embedding
        embedding2: Segundo embedding
        
    Returns:
        Valor de similaridade (0-1)
    """
    if len(embedding1) != len(embedding2):
        raise ValueError("Embeddings devem ter a mesma dimens√£o")
    
    dot_product = sum(a * b for a, b in zip(embedding1, embedding2))
    magnitude1 = sum(a * a for a in embedding1) ** 0.5
    magnitude2 = sum(b * b for b in embedding2) ** 0.5
    
    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0
    
    return max(0.0, min(1.0, dot_product / (magnitude1 * magnitude2)))


# ============================================================================
# UTILIT√ÅRIOS DE PERFORMANCE E MONITORAMENTO
# ============================================================================

def calculate_memory_usage_stats(contexts: List[AgentContext]) -> Dict[str, Any]:
    """
    Calcula estat√≠sticas de uso de mem√≥ria.
    
    Args:
        contexts: Lista de contextos
        
    Returns:
        Dicion√°rio com estat√≠sticas
    """
    if not contexts:
        return {
            'total_contexts': 0,
            'total_size_bytes': 0,
            'avg_size_bytes': 0,
            'total_access_count': 0,
            'avg_access_count': 0
        }
    
    total_size = sum(ctx.data_size_bytes or 0 for ctx in contexts)
    total_access = sum(ctx.access_count for ctx in contexts)
    
    return {
        'total_contexts': len(contexts),
        'total_size_bytes': total_size,
        'avg_size_bytes': total_size / len(contexts),
        'total_access_count': total_access,
        'avg_access_count': total_access / len(contexts),
        'contexts_by_type': {
            ctx_type.value: len([ctx for ctx in contexts if ctx.context_type == ctx_type])
            for ctx_type in ContextType
        }
    }


def identify_cleanup_candidates(contexts: List[AgentContext], 
                               min_priority: int = 8,
                               max_unused_days: int = 7) -> List[AgentContext]:
    """
    Identifica contextos candidatos para limpeza.
    
    Args:
        contexts: Lista de contextos
        min_priority: Prioridade m√≠nima para considerar limpeza
        max_unused_days: M√°ximo de dias sem uso
        
    Returns:
        Lista de contextos candidatos para limpeza
    """
    cutoff_date = datetime.now() - timedelta(days=max_unused_days)
    
    candidates = []
    for context in contexts:
        # Crit√©rios para limpeza:
        # 1. Baixa prioridade E n√£o acessado recentemente
        # 2. OU expirado
        # 3. OU contexto de cache antigo
        
        if context.is_expired():
            candidates.append(context)
        elif (context.priority >= min_priority and 
              context.last_accessed_at < cutoff_date):
            candidates.append(context)
        elif (context.context_type == ContextType.CACHE and 
              context.last_accessed_at < cutoff_date):
            candidates.append(context)
    
    return candidates


def format_memory_summary(stats: Dict[str, Any]) -> str:
    """
    Formata resumo de estat√≠sticas de mem√≥ria.
    
    Args:
        stats: Estat√≠sticas de mem√≥ria
        
    Returns:
        String formatada com resumo
    """
    total_mb = stats['total_size_bytes'] / (1024 * 1024)
    avg_kb = stats['avg_size_bytes'] / 1024
    
    summary = f"""
üìä Resumo da Mem√≥ria dos Agentes:
‚îú‚îÄ Total de contextos: {stats['total_contexts']}
‚îú‚îÄ Tamanho total: {total_mb:.2f} MB
‚îú‚îÄ Tamanho m√©dio: {avg_kb:.2f} KB
‚îú‚îÄ Total de acessos: {stats['total_access_count']}
‚îî‚îÄ M√©dia de acessos: {stats['avg_access_count']:.1f}

üìà Distribui√ß√£o por tipo:
"""
    
    for ctx_type, count in stats['contexts_by_type'].items():
        if count > 0:
            summary += f"‚îú‚îÄ {ctx_type}: {count}\n"
    
    return summary.strip()