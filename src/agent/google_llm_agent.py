"""Agente LLM usando Google Generative AI (Gemini)
===============================================

Este agente utiliza o Google Gemini para anÃ¡lises inteligentes e insights.
Integra com o sistema multiagente para fornecer capacidades avanÃ§adas de NLP.
"""

from __future__ import annotations
import sys
import os
from pathlib import Path

# Adiciona o diretÃ³rio raiz do projeto ao PYTHONPATH
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

from typing import Dict, Any, Optional, List
import json
from dataclasses import dataclass

from src.agent.base_agent import BaseAgent, AgentError
from src.settings import GOOGLE_API_KEY
from src.utils.logging_config import get_logger

# Import condicional do sistema RAG
try:
    from src.embeddings.vector_store import VectorStore
    from src.embeddings.generator import EmbeddingGenerator, EmbeddingProvider
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    VectorStore = None
    EmbeddingGenerator = None

# Import condicional do Google Generative AI
try:
    import google.generativeai as genai
    GOOGLE_AI_AVAILABLE = True
except ImportError:
    GOOGLE_AI_AVAILABLE = False
    genai = None

logger = get_logger(__name__)


@dataclass
class LLMRequest:
    """Request para o LLM."""
    prompt: str
    context: Optional[Dict[str, Any]] = None
    temperature: float = 0.3
    max_tokens: int = 1000
    system_prompt: Optional[str] = None


@dataclass 
class LLMResponse:
    """Resposta do LLM."""
    content: str
    usage: Dict[str, Any]
    model: str
    success: bool = True
    error: Optional[str] = None


class GoogleLLMAgent(BaseAgent):
    """Agente que utiliza Google Generative AI (Gemini) para anÃ¡lises inteligentes."""
    
    def __init__(self, model: str = "gemini-2.0-flash"):
        super().__init__(
            name="google_llm",
            description="Agente LLM usando Google Gemini para anÃ¡lises inteligentes e insights"
        )
        
        self.model_name = model
        self.model = None
        
        # Verificar disponibilidade
        if not GOOGLE_AI_AVAILABLE:
            raise AgentError(
                self.name,
                "Google Generative AI nÃ£o disponÃ­vel. Execute: pip install google-generativeai"
            )
        
        if not GOOGLE_API_KEY:
            raise AgentError(
                self.name, 
                "GOOGLE_API_KEY nÃ£o configurado. Configure em configs/.env"
            )
        
        # Inicializar sistema RAG se disponÃ­vel
        self.rag_enabled = False
        self.vector_store = None
        self.embedding_generator = None
        
        if RAG_AVAILABLE:
            try:
                self.vector_store = VectorStore()
                self.embedding_generator = EmbeddingGenerator(EmbeddingProvider.SENTENCE_TRANSFORMER)
                self.rag_enabled = True
                self.logger.info("RAG integrado ao Google LLM Agent")
            except Exception as e:
                self.logger.warning(f"RAG nÃ£o disponÃ­vel: {e}")
        
        # Configurar e inicializar Google Gemini
        try:
            genai.configure(api_key=GOOGLE_API_KEY)
            self.model = genai.GenerativeModel(model)
            self.logger.info(f"Google LLM inicializado: {model}")
            
        except Exception as e:
            raise AgentError(self.name, f"Erro na inicializaÃ§Ã£o do Google LLM: {e}")
    
    def process(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Processa consulta usando busca RAG + Google Gemini (sistema hÃ­brido).
        
        Fluxo:
        1. Se RAG disponÃ­vel, busca consultas similares no banco vetorial
        2. Se encontrar resposta relevante, usa ela (cache inteligente)  
        3. Se nÃ£o encontrar, chama Google Gemini
        4. Salva consulta + resposta LLM no banco vetorial
        
        Args:
            query: Consulta/prompt para o LLM
            context: Contexto adicional (dados, configuraÃ§Ãµes, etc.)
            
        Returns:
            Resposta estruturada do LLM ou cache
        """
        try:
            # 1. BUSCAR NO CACHE VETORIAL (se disponÃ­vel)
            if self.rag_enabled:
                cached_response = self._search_cached_response(query)
                if cached_response:
                    self.logger.info("ðŸ’¾ Usando resposta em cache (RAG)")
                    return self._build_response(
                        cached_response["content"],
                        metadata={
                            "model": "cache_rag", 
                            "llm_used": False,
                            "cache_used": True,
                            "similarity_score": cached_response.get("similarity", 0.0),
                            "success": True
                        }
                    )
            
            # 2. GERAR NOVA RESPOSTA COM LLM
            self.logger.info("ðŸ¤– Gerando nova resposta com Google Gemini")
            
            # Preparar request
            llm_request = self._prepare_request(query, context)
            
            # Enviar para Google Gemini
            response = self._call_gemini(llm_request)
            
            # 3. SALVAR NO BANCO VETORIAL (se disponÃ­vel)
            if self.rag_enabled and response.success:
                self._save_to_vector_store(query, response.content, context)
            
            # Processar resposta
            return self._build_response(
                response.content,
                metadata={
                    "model": response.model,
                    "usage": response.usage,
                    "llm_used": True,
                    "cache_used": False,
                    "success": response.success
                }
            )
            
        except Exception as e:
            self.logger.error(f"Erro no processamento LLM hÃ­brido: {e}")
            return self._build_response(
                f"Erro no processamento: {str(e)}",
                metadata={"error": True, "llm_used": False, "cache_used": False}
            )
    
    def _prepare_request(self, query: str, context: Optional[Dict[str, Any]]) -> LLMRequest:
        """Prepara request para o LLM com contexto e system prompt."""
        
        # System prompt para anÃ¡lise de dados
        system_prompt = """VocÃª Ã© um especialista em anÃ¡lise de dados e detecÃ§Ã£o de fraudes.
        
Suas responsabilidades:
- Analisar dados CSV e identificar padrÃµes
- Detectar anomalias e possÃ­veis fraudes
- Fornecer insights estratÃ©gicos baseados em dados
- Explicar correlaÃ§Ãµes e tendÃªncias
- Sugerir aÃ§Ãµes para melhorar seguranÃ§a

Diretrizes:
- Seja preciso e baseie-se nos dados fornecidos
- Use linguagem tÃ©cnica mas acessÃ­vel
- Destaque descobertas importantes
- ForneÃ§a recomendaÃ§Ãµes prÃ¡ticas
- Seja conciso mas completo
"""
        
        # Construir prompt com contexto
        if context:
            prompt_parts = [system_prompt, "\nContexto dos dados:"]
            
            # Adicionar informaÃ§Ãµes do contexto
            if "file_path" in context:
                prompt_parts.append(f"Arquivo: {context['file_path']}")
            
            if "data_info" in context:
                data_info = context["data_info"]
                prompt_parts.append(f"DimensÃµes: {data_info.get('rows', 'N/A')} linhas Ã— {data_info.get('columns', 'N/A')} colunas")
            
            if "fraud_data" in context:
                fraud_info = context["fraud_data"] 
                prompt_parts.append(f"Fraudes: {fraud_info.get('count', 0)} de {fraud_info.get('total', 0)} transaÃ§Ãµes")
            
            # Adicionar consulta do usuÃ¡rio
            prompt_parts.extend(["\nConsulta do usuÃ¡rio:", query])
            
            full_prompt = "\n".join(prompt_parts)
        else:
            full_prompt = f"{system_prompt}\n\nConsulta: {query}"
        
        return LLMRequest(
            prompt=full_prompt,
            context=context,
            temperature=0.3,  # Mais determinÃ­stico para anÃ¡lise de dados
            max_tokens=1000
        )
    
    def _call_gemini(self, request: LLMRequest) -> LLMResponse:
        """Chama Google Gemini API."""
        try:
            # Configurar parÃ¢metros de geraÃ§Ã£o
            generation_config = {
                "temperature": request.temperature,
                "max_output_tokens": request.max_tokens,
                "candidate_count": 1,
            }
            
            # Gerar resposta
            response = self.model.generate_content(
                request.prompt,
                generation_config=generation_config
            )
            
            # Extrair conteÃºdo
            content = response.text if response.text else "Sem resposta gerada"
            
            # Metadados de uso (Gemini nÃ£o fornece tokens detalhados via API gratuita)
            usage = {
                "prompt_tokens": len(request.prompt.split()),  # Aproximado
                "completion_tokens": len(content.split()),     # Aproximado  
                "total_tokens": len(request.prompt.split()) + len(content.split())
            }
            
            return LLMResponse(
                content=content,
                usage=usage,
                model=self.model_name,
                success=True
            )
            
        except Exception as e:
            self.logger.error(f"Erro na chamada Gemini: {e}")
            return LLMResponse(
                content=f"Erro na geraÃ§Ã£o: {str(e)}",
                usage={},
                model=self.model_name,
                success=False,
                error=str(e)
            )
    
    def analyze_data_insights(self, data_summary: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa dados e gera insights inteligentes."""
        
        # Construir prompt especializado para anÃ¡lise de dados
        prompt = f"""Analise os seguintes dados e forneÃ§a insights estratÃ©gicos:

RESUMO DOS DADOS:
{json.dumps(data_summary, indent=2, ensure_ascii=False)}

Por favor, forneÃ§a:
1. **Principais Descobertas**: 3-5 insights mais importantes
2. **PadrÃµes Identificados**: TendÃªncias ou correlaÃ§Ãµes relevantes  
3. **Riscos Potenciais**: PossÃ­veis problemas ou anomalias
4. **RecomendaÃ§Ãµes**: 3-5 aÃ§Ãµes estratÃ©gicas especÃ­ficas
5. **PrÃ³ximos Passos**: SugestÃµes para anÃ¡lises adicionais

Formato: Use markdown para estruturar a resposta de forma clara."""
        
        context = {"analysis_type": "data_insights", "data_summary": data_summary}
        
        return self.process(prompt, context)
    
    def detect_fraud_patterns(self, fraud_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa padrÃµes de fraude e fornece insights especializados."""
        
        prompt = f"""Analise os padrÃµes de fraude nos seguintes dados:

DADOS DE FRAUDE:
{json.dumps(fraud_data, indent=2, ensure_ascii=False)}

ForneÃ§a uma anÃ¡lise especializada incluindo:

1. **AnÃ¡lise de Risco**: Avalie o nÃ­vel de risco atual
2. **PadrÃµes de Fraude**: Identifique caracterÃ­sticas comuns das fraudes
3. **Fatores de Risco**: Quais variÃ¡veis mais indicam fraude?
4. **EstratÃ©gias de PrevenÃ§Ã£o**: Como reduzir fraudes futuras?
5. **Alertas CrÃ­ticos**: SituaÃ§Ãµes que requerem atenÃ§Ã£o imediata

Use dados concretos e seja especÃ­fico nas recomendaÃ§Ãµes."""
        
        context = {"analysis_type": "fraud_detection", "fraud_data": fraud_data}
        
        return self.process(prompt, context)
    
    def explain_correlations(self, correlation_data: Dict[str, Any]) -> Dict[str, Any]:
        """Explica correlaÃ§Ãµes de forma acessÃ­vel para negÃ³cios."""
        
        prompt = f"""Explique as correlaÃ§Ãµes nos dados de forma acessÃ­vel para stakeholders de negÃ³cio:

CORRELAÃ‡Ã•ES ENCONTRADAS:
{json.dumps(correlation_data, indent=2, ensure_ascii=False)}

Explique:
1. **O que significam** essas correlaÃ§Ãµes em termos prÃ¡ticos
2. **ImplicaÃ§Ãµes de negÃ³cio** de cada correlaÃ§Ã£o importante
3. **Oportunidades** identificadas atravÃ©s das correlaÃ§Ãµes
4. **Riscos** potenciais revelados pelos dados
5. **AÃ§Ãµes recomendadas** baseadas nessas descobertas

Use linguagem acessÃ­vel e evite jargÃ£o estatÃ­stico excessivo."""
        
        context = {"analysis_type": "correlation_analysis", "correlation_data": correlation_data}
        
        return self.process(prompt, context)

    def _search_cached_response(self, query: str, similarity_threshold: float = 0.8) -> Optional[Dict[str, Any]]:
        """Busca respostas similares no cache vetorial.
        
        Args:
            query: Consulta para buscar
            similarity_threshold: Limite mÃ­nimo de similaridade (0-1)
            
        Returns:
            Resposta em cache se encontrada, senÃ£o None
        """
        if not self.rag_enabled:
            return None
            
        try:
            # Gerar embedding da consulta
            query_result = self.embedding_generator.generate_embedding(query)
            query_embedding = query_result.embedding  # Extrair lista de floats
            
            # Buscar consultas similares
            results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                limit=1,
                similarity_threshold=similarity_threshold
            )
            
            if results and len(results) > 0:
                best_match = results[0]
                self.logger.info(f"ðŸŽ¯ Cache hit! Similaridade: {best_match.similarity_score:.3f}")
                
                return {
                    "content": best_match.metadata.get("response", ""),
                    "similarity": best_match.similarity_score,
                    "original_query": best_match.chunk_text
                }
                
        except Exception as e:
            self.logger.warning(f"Erro na busca de cache: {e}")
            
        return None
    
    def _save_to_vector_store(self, query: str, response: str, context: Optional[Dict[str, Any]] = None) -> bool:
        """Salva consulta e resposta no banco vetorial para cache futuro.
        
        Args:
            query: Consulta original
            response: Resposta do LLM
            context: Contexto adicional
            
        Returns:
            True se salvou com sucesso, False caso contrÃ¡rio
        """
        if not self.rag_enabled:
            return False
            
        try:
            # Preparar metadados
            metadata = {
                "agent": self.name,
                "model": self.model_name,
                "response_content": response,
                "timestamp": self._get_timestamp(),
                "context_keys": list(context.keys()) if context else [],
                "query_type": "llm_analysis"
            }
            
            # Se hÃ¡ contexto de arquivo, incluir  
            if context and "file_path" in context:
                metadata["source_file"] = context["file_path"]
            
            # Gerar e salvar embedding
            query_result = self.embedding_generator.generate_embedding(query)
            query_embedding = query_result.embedding  # Extrair lista de floats
            
            result = self.vector_store.store_embedding(
                query=query,
                response=response,
                embedding=query_embedding
            )
            
            if result:
                self.logger.info(f"ðŸ’¾ Consulta salva no cache vetorial: {query[:50]}...")
                return True
            else:
                self.logger.warning("Falha ao salvar no cache vetorial")
                return False
                
        except Exception as e:
            self.logger.error(f"Erro ao salvar no cache: {e}")
            return False