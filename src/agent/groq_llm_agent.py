"""Agente LLM usando Groq
=======================

Este agente utiliza a API do Groq para an√°lises inteligentes e insights.
Integra com o sistema multiagente para fornecer capacidades avan√ßadas de NLP.
"""

from __future__ import annotations
import sys
import os
from pathlib import Path

# Adiciona o diret√≥rio raiz do projeto ao PYTHONPATH
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

from typing import Dict, Any, Optional, List
import json
from dataclasses import dataclass

from src.agent.base_agent import BaseAgent, AgentError
from src.settings import GROQ_API_KEY
from src.utils.logging_config import get_logger

# Import da biblioteca oficial do Groq
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    Groq = None

# Import condicional do sistema RAG
try:
    from src.embeddings.vector_store import VectorStore
    from src.embeddings.generator import EmbeddingGenerator, EmbeddingProvider
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    VectorStore = None
    EmbeddingGenerator = None

logger = get_logger(__name__)


@dataclass
class GroqRequest:
    """Request para o Groq."""
    prompt: str
    context: Optional[Dict[str, Any]] = None
    temperature: float = 0.3
    max_tokens: int = 1000
    system_prompt: Optional[str] = None


@dataclass 
class GroqResponse:
    """Resposta do Groq."""
    content: str
    usage: Dict[str, Any]
    model: str
    success: bool = True
    error: Optional[str] = None


class GroqLLMAgent(BaseAgent):
    """Agente que utiliza Groq para an√°lises inteligentes."""
    
    def __init__(self, model: str = "llama-3.3-70b-versatile"):
        super().__init__(
            name="groq_llm",
            description="Agente LLM usando Groq para an√°lises inteligentes e insights"
        )
        
        self.model_name = model
        
        # Verificar disponibilidade da biblioteca
        if not GROQ_AVAILABLE:
            raise AgentError(
                self.name,
                "Biblioteca groq n√£o instalada. Execute: pip install groq"
            )
        
        # Verificar API key
        if not GROQ_API_KEY:
            raise AgentError(
                self.name, 
                "GROQ_API_KEY n√£o configurado. Configure em configs/.env"
            )
        
        # Inicializar cliente Groq
        try:
            self.client = Groq(api_key=GROQ_API_KEY)
        except Exception as e:
            raise AgentError(self.name, f"Erro ao inicializar cliente Groq: {e}")
        
        # Inicializar sistema RAG se dispon√≠vel
        self.rag_enabled = False
        self.vector_store = None
        self.embedding_generator = None
        
        if RAG_AVAILABLE:
            try:
                self.vector_store = VectorStore()
                self.embedding_generator = EmbeddingGenerator(EmbeddingProvider.SENTENCE_TRANSFORMER)
                self.rag_enabled = True
                self.logger.info("RAG integrado ao Groq LLM Agent")
            except Exception as e:
                self.logger.warning(f"RAG n√£o dispon√≠vel: {e}")
        
        self.logger.info(f"Groq LLM inicializado: {model}")

    def process(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Processa consulta usando busca RAG + Groq (sistema h√≠brido).
        
        Fluxo:
        1. Se RAG dispon√≠vel, busca consultas similares no banco vetorial
        2. Se encontrar resposta relevante, usa ela (cache inteligente)  
        3. Se n√£o encontrar, chama Groq
        4. Salva consulta + resposta LLM no banco vetorial
        
        Args:
            query: Consulta/prompt para o LLM
            context: Contexto adicional (dados, configura√ß√µes, etc.)
            
        Returns:
            Resposta estruturada do LLM ou cache
        """
        try:
            # 1. BUSCAR NO CACHE VETORIAL (se dispon√≠vel)
            if self.rag_enabled:
                cached_response = self._search_cached_response(query)
                if cached_response:
                    self.logger.info("üíæ Usando resposta em cache (RAG)")
                    return self._build_response(
                        cached_response["content"],
                        metadata={
                            "model": "cache_rag", 
                            "llm_used": False,
                            "cache_used": True,
                            "similarity_score": cached_response.get("similarity", 0.0),
                            "success": True
                        }
                    )
            
            # 2. GERAR NOVA RESPOSTA COM GROQ
            self.logger.info("üöÄ Gerando nova resposta com Groq")
            
            # Preparar request
            groq_request = self._prepare_request(query, context)
            
            # Enviar para Groq
            response = self._call_groq(groq_request)
            
            # 3. SALVAR NO BANCO VETORIAL (se dispon√≠vel)
            if self.rag_enabled and response.success:
                self._save_to_vector_store(query, response.content, context)
            
            # Processar resposta
            return self._build_response(
                response.content,
                metadata={
                    "model": response.model,
                    "usage": response.usage,
                    "llm_used": True,
                    "cache_used": False,
                    "success": response.success
                }
            )
            
        except Exception as e:
            self.logger.error(f"Erro no processamento LLM h√≠brido: {e}")
            return self._build_response(
                f"Erro no processamento: {str(e)}",
                metadata={"error": True, "llm_used": False, "cache_used": False}
            )

    def _prepare_request(self, query: str, context: Optional[Dict[str, Any]]) -> GroqRequest:
        """Prepara request para o LLM com contexto e system prompt."""
        
        # System prompt para an√°lise de dados
        system_prompt = """Voc√™ √© um especialista em an√°lise de dados e detec√ß√£o de fraudes.
        
Suas responsabilidades:
- Analisar dados CSV e identificar padr√µes
- Detectar anomalias e poss√≠veis fraudes
- Fornecer insights estrat√©gicos baseados em dados
- Explicar correla√ß√µes e tend√™ncias
- Sugerir a√ß√µes para melhorar seguran√ßa

Diretrizes:
- Seja preciso e baseie-se nos dados fornecidos
- Use linguagem t√©cnica mas acess√≠vel
- Destaque descobertas importantes
- Forne√ßa recomenda√ß√µes pr√°ticas
- Seja conciso mas completo
"""
        
        # Construir prompt com contexto
        if context:
            prompt_parts = [system_prompt, "\nContexto dos dados:"]
            
            # Adicionar informa√ß√µes do contexto
            if "file_path" in context:
                prompt_parts.append(f"Arquivo: {context['file_path']}")
            
            if "data_info" in context:
                data_info = context["data_info"]
                prompt_parts.append(f"Dimens√µes: {data_info.get('rows', 'N/A')} linhas √ó {data_info.get('columns', 'N/A')} colunas")
            
            if "fraud_data" in context:
                fraud_info = context["fraud_data"] 
                prompt_parts.append(f"Fraudes: {fraud_info.get('count', 0)} de {fraud_info.get('total', 0)} transa√ß√µes")
            
            # Adicionar consulta do usu√°rio
            prompt_parts.extend(["\nConsulta do usu√°rio:", query])
            
            full_prompt = "\n".join(prompt_parts)
        else:
            full_prompt = f"{system_prompt}\n\nConsulta do usu√°rio:\n{query}"
        
        return GroqRequest(
            prompt=full_prompt,
            context=context,
            system_prompt=system_prompt
        )

    def _call_groq(self, request: GroqRequest) -> GroqResponse:
        """Chama a API do Groq usando a biblioteca oficial."""
        try:
            # Preparar mensagens para o chat completion
            messages = [
                {"role": "system", "content": request.system_prompt},
                {"role": "user", "content": request.prompt}
            ]
            
            # Fazer chamada para o Groq
            chat_completion = self.client.chat.completions.create(
                messages=messages,
                model=self.model_name,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
            )
            
            # Extrair resposta
            content = chat_completion.choices[0].message.content
            
            # Construir resposta estruturada
            usage = {
                "prompt_tokens": chat_completion.usage.prompt_tokens if chat_completion.usage else 0,
                "completion_tokens": chat_completion.usage.completion_tokens if chat_completion.usage else 0,
                "total_tokens": chat_completion.usage.total_tokens if chat_completion.usage else 0,
            }
            
            return GroqResponse(
                content=content,
                usage=usage,
                model=self.model_name,
                success=True
            )
            
        except Exception as e:
            self.logger.error(f"Erro na chamada do Groq: {e}")
            return GroqResponse(
                content="",
                usage={},
                model=self.model_name,
                success=False,
                error=str(e)
            )

    def _search_cached_response(self, query: str) -> Optional[Dict[str, Any]]:
        """Busca resposta similar no banco vetorial (cache inteligente)."""
        if not self.rag_enabled:
            return None
            
        try:
            # Gerar embedding da consulta
            query_embedding = self.embedding_generator.generate_embedding(query)
            
            # Buscar respostas similares
            results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                limit=1,
                similarity_threshold=0.85  # Similaridade alta para cache
            )
            
            if results:
                return results[0]
                
        except Exception as e:
            self.logger.warning(f"Erro na busca RAG: {e}")
            
        return None

    def _save_to_vector_store(self, query: str, response: str, context: Optional[Dict[str, Any]]):
        """Salva consulta e resposta no banco vetorial."""
        if not self.rag_enabled:
            return
            
        try:
            # Preparar dados para salvar
            combined_text = f"Consulta: {query}\nResposta: {response}"
            embedding = self.embedding_generator.generate_embedding(combined_text)
            
            # Metadados
            metadata = {
                "query": query,
                "response": response,
                "model": self.model_name,
                "agent": self.name,
                "context": context or {}
            }
            
            # Salvar no banco vetorial
            self.vector_store.add_document(
                text=combined_text,
                embedding=embedding,
                metadata=metadata
            )
            
            self.logger.debug("Resposta salva no banco vetorial")
            
        except Exception as e:
            self.logger.warning(f"Erro ao salvar no banco vetorial: {e}")

    def _build_response(self, content: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Constr√≥i resposta padronizada do agente."""
        return {
            "agent": self.name,
            "content": content,
            "success": metadata.get("success", True),
            "metadata": metadata,
            "type": "llm_response"
        }

    # M√©todos especializados para diferentes tipos de an√°lise
    
    def analyze_data_insights(self, data_summary: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa dados e gera insights espec√≠ficos."""
        query = f"""
        Analise os seguintes dados e forne√ßa insights detalhados:
        
        Resumo dos dados:
        {json.dumps(data_summary, indent=2, ensure_ascii=False)}
        
        Forne√ßa:
        1. Principais padr√µes identificados
        2. Anomalias ou outliers importantes
        3. Correla√ß√µes relevantes
        4. Recomenda√ß√µes baseadas nos dados
        """
        
        return self.process(query, {"data_summary": data_summary})

    def detect_fraud_patterns(self, fraud_data: Dict[str, Any]) -> Dict[str, Any]:
        """Detecta padr√µes de fraude nos dados."""
        query = f"""
        Analise os seguintes dados de fraude e identifique padr√µes:
        
        Dados de fraude:
        {json.dumps(fraud_data, indent=2, ensure_ascii=False)}
        
        Identifique:
        1. Padr√µes comuns em transa√ß√µes fraudulentas
        2. Fatores de risco principais
        3. Estrat√©gias de preven√ß√£o
        4. Indicadores de alerta precoce
        """
        
        return self.process(query, {"fraud_data": fraud_data})

    def explain_correlations(self, correlation_matrix: Dict[str, Any]) -> Dict[str, Any]:
        """Explica correla√ß√µes encontradas nos dados."""
        query = f"""
        Explique as correla√ß√µes encontradas na matriz de correla√ß√£o:
        
        Matriz de correla√ß√£o:
        {json.dumps(correlation_matrix, indent=2, ensure_ascii=False)}
        
        Explique:
        1. Correla√ß√µes mais significativas
        2. Implica√ß√µes pr√°ticas
        3. Poss√≠veis rela√ß√µes causais
        4. Recomenda√ß√µes baseadas nas correla√ß√µes
        """
        
        return self.process(query, {"correlation_matrix": correlation_matrix})

    def get_available_models(self) -> List[str]:
        """Retorna lista de modelos dispon√≠veis no Groq."""
        return [
            "llama-3.3-70b-versatile",  # Modelo principal recomendado
            "llama-3.1-8b-instant",     # Modelo r√°pido
            "meta-llama/llama-guard-4-12b",  # Modelo de modera√ß√£o
            "openai/gpt-oss-120b",      # Modelo experimental
            "openai/gpt-oss-20b"        # Modelo experimental menor
        ]

    def get_model_info(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes sobre o modelo atual."""
        return {
            "model": self.model_name,
            "provider": "Groq",
            "capabilities": [
                "text_generation",
                "data_analysis", 
                "fraud_detection",
                "pattern_recognition",
                "insight_generation"
            ],
            "rag_enabled": self.rag_enabled,
            "api_base": "https://api.groq.com"
        }